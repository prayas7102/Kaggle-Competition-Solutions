{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import (\n",
    "    PowerTransformer,\n",
    "    StandardScaler,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    ")\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Real\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import Ridge, SGDRegressor,ElasticNet, Lasso\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.pipeline import FunctionTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "import re\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from ydata_profiling import ProfileReport\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "excel_file_path = \"./train.csv\"\n",
    "df = pd.read_csv(excel_file_path, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['Listening_Time_minutes'] != 0)]\n",
    "df['Listening_Time_minutes'] = np.log1p(df['Listening_Time_minutes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_eda():\n",
    "    profile = ProfileReport(\n",
    "        pd.concat([df], axis=1),\n",
    "        title=\"Pandas Profiling Report\",\n",
    "        explorative=True,\n",
    "    )\n",
    "    profile.to_file(\"pandas_profiling_report.html\")\n",
    "\n",
    "\n",
    "# gen_eda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Podcast_Name</th>\n",
       "      <th>Episode_Title</th>\n",
       "      <th>Episode_Length_minutes</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Host_Popularity_percentage</th>\n",
       "      <th>Publication_Day</th>\n",
       "      <th>Publication_Time</th>\n",
       "      <th>Guest_Popularity_percentage</th>\n",
       "      <th>Number_of_Ads</th>\n",
       "      <th>Episode_Sentiment</th>\n",
       "      <th>Listening_Time_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>185576</th>\n",
       "      <td>185576</td>\n",
       "      <td>Mind &amp; Body</td>\n",
       "      <td>Episode 38</td>\n",
       "      <td>46.77</td>\n",
       "      <td>Health</td>\n",
       "      <td>29.20</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Morning</td>\n",
       "      <td>91.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>3.325991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456613</th>\n",
       "      <td>456613</td>\n",
       "      <td>Sports Central</td>\n",
       "      <td>Episode 10</td>\n",
       "      <td>102.15</td>\n",
       "      <td>Sports</td>\n",
       "      <td>82.56</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Morning</td>\n",
       "      <td>81.59</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4.256800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342330</th>\n",
       "      <td>342330</td>\n",
       "      <td>Detective Diaries</td>\n",
       "      <td>Episode 56</td>\n",
       "      <td>77.72</td>\n",
       "      <td>True Crime</td>\n",
       "      <td>45.45</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>38.55</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>3.744219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80730</th>\n",
       "      <td>80730</td>\n",
       "      <td>Athlete's Arena</td>\n",
       "      <td>Episode 21</td>\n",
       "      <td>72.75</td>\n",
       "      <td>Sports</td>\n",
       "      <td>68.30</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>Morning</td>\n",
       "      <td>34.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>3.708034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158689</th>\n",
       "      <td>158689</td>\n",
       "      <td>Sports Weekly</td>\n",
       "      <td>Episode 12</td>\n",
       "      <td>29.61</td>\n",
       "      <td>Sports</td>\n",
       "      <td>30.08</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Evening</td>\n",
       "      <td>14.60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>2.673712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id       Podcast_Name Episode_Title  Episode_Length_minutes  \\\n",
       "185576  185576        Mind & Body    Episode 38                   46.77   \n",
       "456613  456613     Sports Central    Episode 10                  102.15   \n",
       "342330  342330  Detective Diaries    Episode 56                   77.72   \n",
       "80730    80730    Athlete's Arena    Episode 21                   72.75   \n",
       "158689  158689      Sports Weekly    Episode 12                   29.61   \n",
       "\n",
       "             Genre  Host_Popularity_percentage Publication_Day  \\\n",
       "185576      Health                       29.20          Monday   \n",
       "456613      Sports                       82.56        Saturday   \n",
       "342330  True Crime                       45.45        Saturday   \n",
       "80730       Sports                       68.30          Sunday   \n",
       "158689      Sports                       30.08          Friday   \n",
       "\n",
       "       Publication_Time  Guest_Popularity_percentage  Number_of_Ads  \\\n",
       "185576          Morning                        91.63            0.0   \n",
       "456613          Morning                        81.59            3.0   \n",
       "342330        Afternoon                        38.55            3.0   \n",
       "80730           Morning                        34.96            0.0   \n",
       "158689          Evening                        14.60            1.0   \n",
       "\n",
       "       Episode_Sentiment  Listening_Time_minutes  \n",
       "185576           Neutral                3.325991  \n",
       "456613           Neutral                4.256800  \n",
       "342330          Negative                3.744219  \n",
       "80730            Neutral                3.708034  \n",
       "158689          Negative                2.673712  "
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Episode_Length_minutes</th>\n",
       "      <th>Host_Popularity_percentage</th>\n",
       "      <th>Guest_Popularity_percentage</th>\n",
       "      <th>Number_of_Ads</th>\n",
       "      <th>Listening_Time_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>741449.000000</td>\n",
       "      <td>655066.000000</td>\n",
       "      <td>741449.000000</td>\n",
       "      <td>598546.000000</td>\n",
       "      <td>741448.000000</td>\n",
       "      <td>741449.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>374957.717282</td>\n",
       "      <td>65.185957</td>\n",
       "      <td>59.836673</td>\n",
       "      <td>52.328988</td>\n",
       "      <td>1.348355</td>\n",
       "      <td>3.619211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>216507.054088</td>\n",
       "      <td>32.568739</td>\n",
       "      <td>22.874244</td>\n",
       "      <td>28.493094</td>\n",
       "      <td>1.152670</td>\n",
       "      <td>0.777568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>187430.000000</td>\n",
       "      <td>36.730000</td>\n",
       "      <td>39.380000</td>\n",
       "      <td>28.340000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.213869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>374926.000000</td>\n",
       "      <td>64.420000</td>\n",
       "      <td>60.020000</td>\n",
       "      <td>53.780000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.802549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>562461.000000</td>\n",
       "      <td>94.330000</td>\n",
       "      <td>79.500000</td>\n",
       "      <td>76.760000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.190771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>749999.000000</td>\n",
       "      <td>325.240000</td>\n",
       "      <td>119.460000</td>\n",
       "      <td>119.910000</td>\n",
       "      <td>103.910000</td>\n",
       "      <td>4.795543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id  Episode_Length_minutes  Host_Popularity_percentage  \\\n",
       "count  741449.000000           655066.000000               741449.000000   \n",
       "mean   374957.717282               65.185957                   59.836673   \n",
       "std    216507.054088               32.568739                   22.874244   \n",
       "min         0.000000                0.000000                    1.300000   \n",
       "25%    187430.000000               36.730000                   39.380000   \n",
       "50%    374926.000000               64.420000                   60.020000   \n",
       "75%    562461.000000               94.330000                   79.500000   \n",
       "max    749999.000000              325.240000                  119.460000   \n",
       "\n",
       "       Guest_Popularity_percentage  Number_of_Ads  Listening_Time_minutes  \n",
       "count                598546.000000  741448.000000           741449.000000  \n",
       "mean                     52.328988       1.348355                3.619211  \n",
       "std                      28.493094       1.152670                0.777568  \n",
       "min                       0.000000       0.000000                0.000560  \n",
       "25%                      28.340000       0.000000                3.213869  \n",
       "50%                      53.780000       1.000000                3.802549  \n",
       "75%                      76.760000       2.000000                4.190771  \n",
       "max                     119.910000     103.910000                4.795543  "
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 120984\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(subset=[\n",
    "    \"Episode_Length_minutes\",\n",
    "    # \"Guest_Popularity_percentage\"\n",
    "])\n",
    "print(df[\"Episode_Length_minutes\"].isnull().sum(), df[\"Guest_Popularity_percentage\"].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, outlier_dict):\n",
    "    for distribution, category in outlier_dict.items():\n",
    "        if distribution == \"normal\":\n",
    "            for cat in category:\n",
    "                upper_limit = df[cat].mean() + 3 * df[cat].std()\n",
    "                lower_limit = df[cat].mean() - 3 * df[cat].std()\n",
    "                print(cat, upper_limit, lower_limit)\n",
    "                # capping\n",
    "                df[cat] = np.where(df[cat] > upper_limit,upper_limit,np.where(df[cat] < lower_limit, lower_limit, df[cat]))\n",
    "                # Trimming\n",
    "                # df = df[(df[cat] < upper_limit) & (df[cat] > lower_limit)]\n",
    "        elif distribution == \"skew\":\n",
    "            for cat in category:\n",
    "                percentile25 = df[cat].quantile(0.25)\n",
    "                percentile75 = df[cat].quantile(0.75)\n",
    "                iqr = percentile75 - percentile25\n",
    "                upper_limit = percentile75 + 1.5 * iqr\n",
    "                lower_limit = percentile25 - 1.5 * iqr\n",
    "                print(cat, upper_limit, lower_limit)\n",
    "                # capping\n",
    "                df[cat] = np.where(\n",
    "                    df[cat] > upper_limit,\n",
    "                    upper_limit,\n",
    "                    np.where(df[cat] < lower_limit, lower_limit, df[cat]),\n",
    "                )\n",
    "                # Trimming\n",
    "                # df = df[(df[cat] < upper_limit) & (df[cat] > lower_limit)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode_Length_minutes 180.73000000000002 -49.67000000000001\n",
      "Host_Popularity_percentage 139.59 -20.650000000000006\n",
      "Guest_Popularity_percentage 149.91 -44.809999999999995\n",
      "Number_of_Ads 5.0 -3.0\n",
      "Listening_Time_minutes 5.665705649357011 1.7496219926878542\n"
     ]
    }
   ],
   "source": [
    "outlier_dict = {\n",
    "    \"normal\": [],\n",
    "    \"skew\": ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Listening_Time_minutes']\n",
    "}\n",
    "\n",
    "\n",
    "def pre_process(df):\n",
    "    df[\"Publication_Day\"] = (\n",
    "        df[\"Publication_Day\"]\n",
    "        .map(\n",
    "            { \"Monday\": 0, \"Tuesday\": 1, \"Wednesday\": 2, \"Thursday\": 3, \"Friday\": 4, \"Saturday\": 5, \"Sunday\": 6 }\n",
    "        )\n",
    "        .fillna(0)\n",
    "        .astype(int)\n",
    "    )\n",
    "    df[\"Publication_Time\"] = (\n",
    "        df[\"Publication_Time\"]\n",
    "        .map({\"Morning\": 0, \"Afternoon\": 1, \"Evening\": 2, \"Night\": 3})\n",
    "        .fillna(0)\n",
    "        .astype(int)\n",
    "    )\n",
    "    df[\"Episode_Sentiment\"] = (\n",
    "        df[\"Episode_Sentiment\"]\n",
    "        .map({\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2})\n",
    "        .fillna(0)\n",
    "        .astype(int)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "df = pre_process(df)\n",
    "df = remove_outliers(df, outlier_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Number_of_Episodes'] = df.groupby(['Podcast_Name', 'Genre'])['Podcast_Name'].transform('count')\n",
    "episode_counts_mapping = df.groupby(['Podcast_Name', 'Genre']).size().to_dict()\n",
    "df.to_csv(\"df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(524052, 9)\n"
     ]
    }
   ],
   "source": [
    "# Define features and target\n",
    "def get_X_Y(df):\n",
    "    X = df.drop(columns=[\"id\", \"Listening_Time_minutes\", \"Episode_Title\", \"Podcast_Name\"])\n",
    "    Y = df[\"Listening_Time_minutes\"]\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "X, Y = get_X_Y(df)\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.20, random_state=5\n",
    ")\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical_features ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Number_of_Episodes']\n",
      "categorical_feat_nom ['Genre']\n",
      "categorical_feat_ord ['Publication_Day', 'Publication_Time', 'Episode_Sentiment']\n"
     ]
    }
   ],
   "source": [
    "# Get the list of categorical column names\n",
    "categories_order = {\n",
    "    \"Publication_Day\": sorted(list(df[\"Publication_Day\"].unique())),\n",
    "    \"Publication_Time\": sorted(list(df[\"Publication_Time\"].unique())),\n",
    "    \"Episode_Sentiment\": sorted(list(df[\"Episode_Sentiment\"].unique())),\n",
    "}\n",
    "categorical_feat_ord = list(categories_order.keys())\n",
    "categorical_feat_nom = [ \"Genre\"]\n",
    "categorical = categorical_feat_nom + categorical_feat_ord\n",
    "numerical_features = [col for col in X_train.columns if col not in categorical]\n",
    "print('numerical_features', numerical_features)\n",
    "print('categorical_feat_nom', categorical_feat_nom)\n",
    "print('categorical_feat_ord', categorical_feat_ord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate transformers for categorical and numerical features\n",
    "\n",
    "# trf = FunctionTransformer(np.log1p, validate=True)\n",
    "trf = PowerTransformer()\n",
    "# trf = FunctionTransformer(np.sqrt, validate=True)\n",
    "# trf = FunctionTransformer(np.sin)\n",
    "# trf = StandardScaler()\n",
    "# trf = MinMaxScaler()\n",
    "# Add Polynomial Features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "\n",
    "numerical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", IterativeImputer(estimator=LGBMRegressor(verbose=-1, n_estimators=100, random_state=42), max_iter=10, random_state=42)),\n",
    "        # (\"imputer\", IterativeImputer(estimator=Ridge(alpha=0.1), max_iter=10, random_state=42)),\n",
    "        # (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        # (\"poly\", poly),\n",
    "        (\"log\", trf),\n",
    "    ]\n",
    ")\n",
    "categorical_transformer_onehot = Pipeline(\n",
    "    steps=[\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "categorical_transformer_ordinal = Pipeline(\n",
    "    steps=[\n",
    "        (\"ord\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", categorical_transformer_onehot, categorical_feat_nom),\n",
    "        (\"cat_1\", categorical_transformer_ordinal, categorical_feat_ord),\n",
    "        (\"num\", numerical_transformer, numerical_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the correlation matrix\n",
    "# correlation_matrix = df.corr()\n",
    "\n",
    "# # Save the correlation matrix to a CSV file\n",
    "# correlation_matrix.to_csv('correlation_matrix.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0587786390171736\n",
      "Root Mean Squared Error: 0.24244306345443997\n",
      "R² Score: 0.8907\n",
      "Adjusted R² Score: 0.8907\n"
     ]
    }
   ],
   "source": [
    "def test_score(pipeline):\n",
    "    # Evaluate the model\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    mse = mean_squared_error(Y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"Root Mean Squared Error: {rmse}\")\n",
    "    r2 = r2_score(Y_test, y_pred)\n",
    "    n = len(Y_test)  # number of samples\n",
    "    k = X_train.shape[1]  # number of features\n",
    "    adj_r2 = 1 - (1 - r2) * ((n - 1) / (n - k - 1))\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    print(f\"Adjusted R² Score: {adj_r2:.4f}\")\n",
    "\n",
    "model = Ridge(alpha=0.1)\n",
    "# model = XGBRegressor(objective='reg:squarederror',n_estimators=100,learning_rate=0.1,max_depth=3,random_state=42)\n",
    "model = LGBMRegressor(verbose=-1, objective='regression',n_estimators=100,learning_rate=0.1,max_depth=-1,random_state=42)\n",
    "pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"model\", model)])\n",
    "pipeline.fit(X_train, Y_train)\n",
    "test_score(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as model.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save the fitted pipeline as a .pkl file\n",
    "filename_pkl = \"model.pkl\"\n",
    "pickle.dump(pipeline, open(filename_pkl, \"wb\"))\n",
    "print(f\"Model saved as {filename_pkl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved as 'submission.csv'\n"
     ]
    }
   ],
   "source": [
    "# Define the columns expected by the model\n",
    "column_names = X_train.columns\n",
    "\n",
    "def test_preprocess(df):\n",
    "    df['Number_of_Episodes'] = df.apply(lambda row: episode_counts_mapping.get((row['Podcast_Name'], row['Genre']), 0), axis=1)\n",
    "    return df\n",
    "\n",
    "def generate_submission(test_file):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(test_file)\n",
    "    df = pd.DataFrame(df)\n",
    "    # Replace empty strings with NaN\n",
    "    df.replace(\"\", np.nan, inplace=True)\n",
    "    df = pre_process(df)\n",
    "    df = test_preprocess(df)\n",
    "    # Select the relevant columns\n",
    "    filtered_df = df[column_names]\n",
    "    predictions = pipeline.predict(filtered_df)\n",
    "    # Load the original test file to keep the PassengerId column\n",
    "    original_df = pd.read_csv(test_file)\n",
    "    original_df[\"Listening_Time_minutes\"] = predictions\n",
    "    original_df[\"Listening_Time_minutes\"] = np.expm1(\n",
    "        original_df[\"Listening_Time_minutes\"]\n",
    "    )\n",
    "    # Save the results to a new CSV file\n",
    "    submission_df = original_df[[\"id\", \"Listening_Time_minutes\"]]\n",
    "    submission_df.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"Submission file saved as 'submission.csv'\")\n",
    "\n",
    "\n",
    "# Generate the submission\n",
    "test_file = \"test.csv\"\n",
    "generate_submission(test_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
