{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import (\n",
    "    PowerTransformer,\n",
    "    StandardScaler,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    ")\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Real\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import Ridge, SGDRegressor,ElasticNet, Lasso\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.pipeline import FunctionTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "import re\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from ydata_profiling import ProfileReport\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "excel_file_path = \"./train.csv\"\n",
    "df = pd.read_csv(excel_file_path, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['Listening_Time_minutes'] != 0)]\n",
    "df['Listening_Time_minutes'] = np.log1p(df['Listening_Time_minutes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_eda():\n",
    "    profile = ProfileReport(\n",
    "        pd.concat([df], axis=1),\n",
    "        title=\"Pandas Profiling Report\",\n",
    "        explorative=True,\n",
    "    )\n",
    "    profile.to_file(\"pandas_profiling_report.html\")\n",
    "\n",
    "\n",
    "# gen_eda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Podcast_Name</th>\n",
       "      <th>Episode_Title</th>\n",
       "      <th>Episode_Length_minutes</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Host_Popularity_percentage</th>\n",
       "      <th>Publication_Day</th>\n",
       "      <th>Publication_Time</th>\n",
       "      <th>Guest_Popularity_percentage</th>\n",
       "      <th>Number_of_Ads</th>\n",
       "      <th>Episode_Sentiment</th>\n",
       "      <th>Listening_Time_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>133103</th>\n",
       "      <td>133103</td>\n",
       "      <td>Game Day</td>\n",
       "      <td>Episode 72</td>\n",
       "      <td>103.58</td>\n",
       "      <td>Sports</td>\n",
       "      <td>40.58</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>Morning</td>\n",
       "      <td>65.79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>4.216767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281957</th>\n",
       "      <td>281957</td>\n",
       "      <td>Study Sessions</td>\n",
       "      <td>Episode 53</td>\n",
       "      <td>59.91</td>\n",
       "      <td>Education</td>\n",
       "      <td>32.72</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>3.605093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679566</th>\n",
       "      <td>679566</td>\n",
       "      <td>Gadget Geek</td>\n",
       "      <td>Episode 4</td>\n",
       "      <td>41.49</td>\n",
       "      <td>Technology</td>\n",
       "      <td>95.92</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>Morning</td>\n",
       "      <td>95.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>3.652114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519158</th>\n",
       "      <td>519158</td>\n",
       "      <td>Fitness First</td>\n",
       "      <td>Episode 87</td>\n",
       "      <td>14.27</td>\n",
       "      <td>Health</td>\n",
       "      <td>85.03</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Night</td>\n",
       "      <td>9.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2.418623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495171</th>\n",
       "      <td>495171</td>\n",
       "      <td>Criminal Minds</td>\n",
       "      <td>Episode 53</td>\n",
       "      <td>65.04</td>\n",
       "      <td>True Crime</td>\n",
       "      <td>49.62</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Evening</td>\n",
       "      <td>68.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>3.415718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    Podcast_Name Episode_Title  Episode_Length_minutes  \\\n",
       "133103  133103        Game Day    Episode 72                  103.58   \n",
       "281957  281957  Study Sessions    Episode 53                   59.91   \n",
       "679566  679566     Gadget Geek     Episode 4                   41.49   \n",
       "519158  519158   Fitness First    Episode 87                   14.27   \n",
       "495171  495171  Criminal Minds    Episode 53                   65.04   \n",
       "\n",
       "             Genre  Host_Popularity_percentage Publication_Day  \\\n",
       "133103      Sports                       40.58          Sunday   \n",
       "281957   Education                       32.72          Friday   \n",
       "679566  Technology                       95.92          Sunday   \n",
       "519158      Health                       85.03          Friday   \n",
       "495171  True Crime                       49.62         Tuesday   \n",
       "\n",
       "       Publication_Time  Guest_Popularity_percentage  Number_of_Ads  \\\n",
       "133103          Morning                        65.79            0.0   \n",
       "281957        Afternoon                          NaN            1.0   \n",
       "679566          Morning                        95.75            0.0   \n",
       "519158            Night                         9.48            0.0   \n",
       "495171          Evening                        68.00            0.0   \n",
       "\n",
       "       Episode_Sentiment  Listening_Time_minutes  \n",
       "133103          Positive                4.216767  \n",
       "281957          Negative                3.605093  \n",
       "679566          Negative                3.652114  \n",
       "519158           Neutral                2.418623  \n",
       "495171           Neutral                3.415718  "
      ]
     },
     "execution_count": 709,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Episode_Length_minutes</th>\n",
       "      <th>Host_Popularity_percentage</th>\n",
       "      <th>Guest_Popularity_percentage</th>\n",
       "      <th>Number_of_Ads</th>\n",
       "      <th>Listening_Time_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>741449.000000</td>\n",
       "      <td>655066.000000</td>\n",
       "      <td>741449.000000</td>\n",
       "      <td>598546.000000</td>\n",
       "      <td>741448.000000</td>\n",
       "      <td>741449.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>374957.717282</td>\n",
       "      <td>65.185957</td>\n",
       "      <td>59.836673</td>\n",
       "      <td>52.328988</td>\n",
       "      <td>1.348355</td>\n",
       "      <td>3.619211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>216507.054088</td>\n",
       "      <td>32.568739</td>\n",
       "      <td>22.874244</td>\n",
       "      <td>28.493094</td>\n",
       "      <td>1.152670</td>\n",
       "      <td>0.777568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>187430.000000</td>\n",
       "      <td>36.730000</td>\n",
       "      <td>39.380000</td>\n",
       "      <td>28.340000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.213869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>374926.000000</td>\n",
       "      <td>64.420000</td>\n",
       "      <td>60.020000</td>\n",
       "      <td>53.780000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.802549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>562461.000000</td>\n",
       "      <td>94.330000</td>\n",
       "      <td>79.500000</td>\n",
       "      <td>76.760000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.190771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>749999.000000</td>\n",
       "      <td>325.240000</td>\n",
       "      <td>119.460000</td>\n",
       "      <td>119.910000</td>\n",
       "      <td>103.910000</td>\n",
       "      <td>4.795543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id  Episode_Length_minutes  Host_Popularity_percentage  \\\n",
       "count  741449.000000           655066.000000               741449.000000   \n",
       "mean   374957.717282               65.185957                   59.836673   \n",
       "std    216507.054088               32.568739                   22.874244   \n",
       "min         0.000000                0.000000                    1.300000   \n",
       "25%    187430.000000               36.730000                   39.380000   \n",
       "50%    374926.000000               64.420000                   60.020000   \n",
       "75%    562461.000000               94.330000                   79.500000   \n",
       "max    749999.000000              325.240000                  119.460000   \n",
       "\n",
       "       Guest_Popularity_percentage  Number_of_Ads  Listening_Time_minutes  \n",
       "count                598546.000000  741448.000000           741449.000000  \n",
       "mean                     52.328988       1.348355                3.619211  \n",
       "std                      28.493094       1.152670                0.777568  \n",
       "min                       0.000000       0.000000                0.000560  \n",
       "25%                      28.340000       0.000000                3.213869  \n",
       "50%                      53.780000       1.000000                3.802549  \n",
       "75%                      76.760000       2.000000                4.190771  \n",
       "max                     119.910000     103.910000                4.795543  "
      ]
     },
     "execution_count": 710,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 120984\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(subset=[\n",
    "    \"Episode_Length_minutes\",\n",
    "    # \"Guest_Popularity_percentage\"\n",
    "])\n",
    "print(df[\"Episode_Length_minutes\"].isnull().sum(), df[\"Guest_Popularity_percentage\"].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, outlier_dict):\n",
    "    for distribution, category in outlier_dict.items():\n",
    "        if distribution == \"normal\":\n",
    "            for cat in category:\n",
    "                upper_limit = df[cat].mean() + 3 * df[cat].std()\n",
    "                lower_limit = df[cat].mean() - 3 * df[cat].std()\n",
    "                print(cat, upper_limit, lower_limit)\n",
    "                # capping\n",
    "                # df[cat] = np.where(df[cat] > upper_limit,upper_limit,np.where(df[cat] < lower_limit, lower_limit, df[cat]))\n",
    "                # Trimming\n",
    "                df = df[(df[cat] < upper_limit) & (df[cat] > lower_limit)]\n",
    "        elif distribution == \"skew\":\n",
    "            for cat in category:\n",
    "                percentile25 = df[cat].quantile(0.25)\n",
    "                percentile75 = df[cat].quantile(0.75)\n",
    "                iqr = percentile75 - percentile25\n",
    "                upper_limit = percentile75 + 1.5 * iqr\n",
    "                lower_limit = percentile25 - 1.5 * iqr\n",
    "                print(cat, upper_limit, lower_limit)\n",
    "                # capping\n",
    "                # df[cat] = np.where(\n",
    "                #     df[cat] > upper_limit,\n",
    "                #     upper_limit,\n",
    "                #     np.where(df[cat] < lower_limit, lower_limit, df[cat]),\n",
    "                # )\n",
    "                # Trimming\n",
    "                df = df[(df[cat] < upper_limit) & (df[cat] > lower_limit)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_dict = {\n",
    "    \"normal\": [],\n",
    "    \"skew\": [],\n",
    "}\n",
    "\n",
    "\n",
    "def pre_process(df):\n",
    "    df[\"Publication_Day\"] = (\n",
    "        df[\"Publication_Day\"]\n",
    "        .map(\n",
    "            { \"Monday\": 0, \"Tuesday\": 1, \"Wednesday\": 2, \"Thursday\": 3, \"Friday\": 4, \"Saturday\": 5, \"Sunday\": 6 }\n",
    "        )\n",
    "        .fillna(0)\n",
    "        .astype(int)\n",
    "    )\n",
    "    df[\"Publication_Time\"] = (\n",
    "        df[\"Publication_Time\"]\n",
    "        .map({\"Morning\": 0, \"Afternoon\": 1, \"Evening\": 2, \"Night\": 3})\n",
    "        .fillna(0)\n",
    "        .astype(int)\n",
    "    )\n",
    "    df[\"Episode_Sentiment\"] = (\n",
    "        df[\"Episode_Sentiment\"]\n",
    "        .map({\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2})\n",
    "        .fillna(0)\n",
    "        .astype(int)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "df = pre_process(df)\n",
    "df = remove_outliers(df, outlier_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Number_of_Episodes'] = df.groupby(['Podcast_Name', 'Genre'])['Podcast_Name'].transform('count')\n",
    "df.to_csv(\"df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(524052, 10)\n"
     ]
    }
   ],
   "source": [
    "# Define features and target\n",
    "def get_X_Y(df):\n",
    "    X = df.drop(columns=[\"id\", \"Listening_Time_minutes\", \"Episode_Title\"])\n",
    "    Y = df[\"Listening_Time_minutes\"]\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "X, Y = get_X_Y(df)\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.20, random_state=5\n",
    ")\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical_features ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Number_of_Episodes']\n",
      "categorical_feat_nom ['Podcast_Name', 'Genre']\n",
      "categorical_feat_ord ['Publication_Day', 'Publication_Time', 'Episode_Sentiment']\n"
     ]
    }
   ],
   "source": [
    "# Get the list of categorical column names\n",
    "categories_order = {\n",
    "    \"Publication_Day\": sorted(list(df[\"Publication_Day\"].unique())),\n",
    "    \"Publication_Time\": sorted(list(df[\"Publication_Time\"].unique())),\n",
    "    \"Episode_Sentiment\": sorted(list(df[\"Episode_Sentiment\"].unique())),\n",
    "}\n",
    "categorical_feat_ord = list(categories_order.keys())\n",
    "categorical_feat_nom = [ \"Podcast_Name\", \"Genre\"]\n",
    "categorical = categorical_feat_nom + categorical_feat_ord\n",
    "numerical_features = [col for col in X_train.columns if col not in categorical]\n",
    "print('numerical_features', numerical_features)\n",
    "print('categorical_feat_nom', categorical_feat_nom)\n",
    "print('categorical_feat_ord', categorical_feat_ord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate transformers for categorical and numerical features\n",
    "\n",
    "# trf = FunctionTransformer(np.log1p, validate=True)\n",
    "trf = PowerTransformer()\n",
    "# trf = FunctionTransformer(np.sqrt, validate=True)\n",
    "# trf = FunctionTransformer(np.sin)\n",
    "# trf = StandardScaler()\n",
    "# trf = MinMaxScaler()\n",
    "# Add Polynomial Features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "\n",
    "numerical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")), \n",
    "        (\"poly\", poly),\n",
    "        (\"log\", trf),\n",
    "    ]\n",
    ")\n",
    "categorical_transformer_onehot = Pipeline(\n",
    "    steps=[\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "categorical_transformer_ordinal = Pipeline(\n",
    "    steps=[\n",
    "        (\"ord\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", categorical_transformer_onehot, categorical_feat_nom),\n",
    "        (\"cat_1\", categorical_transformer_ordinal, categorical_feat_ord),\n",
    "        (\"num\", numerical_transformer, numerical_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the correlation matrix\n",
    "# correlation_matrix = df.corr()\n",
    "\n",
    "# # Save the correlation matrix to a CSV file\n",
    "# correlation_matrix.to_csv('correlation_matrix.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0780902479340377\n",
      "Root Mean Squared Error: 0.2794463238871424\n",
      "R² Score: 0.8721\n",
      "Adjusted R² Score: 0.8721\n"
     ]
    }
   ],
   "source": [
    "def adjusted_r2_score(pipeline):\n",
    "    # Evaluate the model\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    mse = mean_squared_error(Y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"Root Mean Squared Error: {rmse}\")\n",
    "    r2 = r2_score(Y_test, y_pred)\n",
    "    n = len(Y_test)  # number of samples\n",
    "    k = X_train.shape[1]  # number of features\n",
    "    adj_r2 = 1 - (1 - r2) * ((n - 1) / (n - k - 1))\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    print(f\"Adjusted R² Score: {adj_r2:.4f}\")\n",
    "\n",
    "model = Ridge(alpha=0.1)\n",
    "# model = XGBRegressor(objective='reg:squarederror',n_estimators=100,learning_rate=0.1,max_depth=3,random_state=42)\n",
    "model = LGBMRegressor(objective='regression',n_estimators=100,learning_rate=0.1,max_depth=-1,random_state=42)\n",
    "pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"model\", model)])\n",
    "pipeline.fit(X_train, Y_train)\n",
    "adjusted_r2_score(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as model.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save the fitted pipeline as a .pkl file\n",
    "filename_pkl = \"model.pkl\"\n",
    "pickle.dump(pipeline, open(filename_pkl, \"wb\"))\n",
    "print(f\"Model saved as {filename_pkl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved as 'submission.csv'\n"
     ]
    }
   ],
   "source": [
    "# Define the columns expected by the model\n",
    "column_names = X_train.columns\n",
    "\n",
    "def test_preprocess(df):\n",
    "    episode_counts_mapping = df.groupby(['Podcast_Name', 'Genre']).size().to_dict()\n",
    "    df['Number_of_Episodes'] = df.apply(lambda row: episode_counts_mapping.get((row['Podcast_Name'], row['Genre']), 0), axis=1)\n",
    "    return df\n",
    "\n",
    "def generate_submission(test_file):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(test_file)\n",
    "    df = pd.DataFrame(df)\n",
    "    # Replace empty strings with NaN\n",
    "    df.replace(\"\", np.nan, inplace=True)\n",
    "    df = pre_process(df)\n",
    "    df = test_preprocess(df)\n",
    "    # Select the relevant columns\n",
    "    filtered_df = df[column_names]\n",
    "    predictions = pipeline.predict(filtered_df)\n",
    "    # Load the original test file to keep the PassengerId column\n",
    "    original_df = pd.read_csv(test_file)\n",
    "    original_df[\"Listening_Time_minutes\"] = predictions\n",
    "    original_df[\"Listening_Time_minutes\"] = np.expm1(\n",
    "        original_df[\"Listening_Time_minutes\"]\n",
    "    )\n",
    "    # Save the results to a new CSV file\n",
    "    submission_df = original_df[[\"id\", \"Listening_Time_minutes\"]]\n",
    "    submission_df.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"Submission file saved as 'submission.csv'\")\n",
    "\n",
    "\n",
    "# Generate the submission\n",
    "test_file = \"test.csv\"\n",
    "generate_submission(test_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
