{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import (\n",
    "    PowerTransformer,\n",
    "    StandardScaler,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    ")\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Real\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "from sklearn.pipeline import FunctionTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "import re\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "excel_file_path = \"./train.csv\"\n",
    "df = pd.read_csv(excel_file_path, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Get unique elements for each column\n",
    "# column = df.columns\n",
    "# column=[\"Sleep Duration\"]\n",
    "# for x in column:\n",
    "#     print(\"feature: \", x)\n",
    "#     print(\"value count\", df[x].value_counts())\n",
    "#     print(\"unique values\", len(df[x].unique()))\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, outlier_dict):\n",
    "    for distribution, category in outlier_dict.items():\n",
    "        if distribution == \"normal\":\n",
    "            for cat in category:\n",
    "                upper_limit = df[cat].mean() + 3 * df[cat].std()\n",
    "                lower_limit = df[cat].mean() - 3 * df[cat].std()\n",
    "                print(cat, upper_limit, lower_limit)\n",
    "                # capping\n",
    "                # df[cat] = np.where(df[cat] > upper_limit,upper_limit,np.where(df[cat] < lower_limit, lower_limit, df[cat]))\n",
    "                # Trimming\n",
    "                df = df[(df[cat] < upper_limit) & (df[cat] > lower_limit)]\n",
    "        elif distribution == \"skew\":\n",
    "            for cat in category:\n",
    "                percentile25 = df[cat].quantile(0.25)\n",
    "                percentile75 = df[cat].quantile(0.75)\n",
    "                iqr = percentile75 - percentile25\n",
    "                upper_limit = percentile75 + 1.5 * iqr\n",
    "                lower_limit = percentile25 - 1.5 * iqr\n",
    "                print(cat, upper_limit, lower_limit)\n",
    "                # capping\n",
    "                # df[cat] = np.where(\n",
    "                #     df[cat] > upper_limit,\n",
    "                #     upper_limit,\n",
    "                #     np.where(df[cat] < lower_limit, lower_limit, df[cat]),\n",
    "                # )\n",
    "                # Trimming\n",
    "                df = df[(df[cat] < upper_limit) & (df[cat] > lower_limit)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_dict = {\n",
    "    \"normal\": [],\n",
    "    \"skew\": [],\n",
    "}\n",
    "\n",
    "def reduce_engine(df: pd.DataFrame)->pd.DataFrame:\n",
    "    \n",
    "    def extract_capacity(x: str)->float:\n",
    "        '''Extracts the volume (mentioned at 3.14L, 3.14 Litres, 3 L, 3. L)'''\n",
    "        matchL = re.search( r'([.\\d]+)\\s*(?:L|Litres|litres|.Litres)', x)\n",
    "        if bool(matchL):\n",
    "            capacity = str(matchL.group(0))\n",
    "            return float(re.findall(r\"[-+]?\\d*\\.*\\d+\", capacity)[0])\n",
    "        return np.nan\n",
    "    \n",
    "    def extract_horse_power(x: str)->float:        \n",
    "        '''Extracts the HorsePower (mentioned at 3.14HP, 3.14 HP, 3 HP, 3. HP)'''\n",
    "        matchHP = re.search( r'([.\\d]+)\\s*(?:HP| HP|  HP|.HP)', x)\n",
    "        if bool(matchHP):\n",
    "            horsePower = str(matchHP.group(0))\n",
    "            return float(re.findall(r\"[-+]?\\d*\\.*\\d+\", horsePower)[0])\n",
    "        return np.nan\n",
    "    \n",
    "    def extract_cylinders(x: str)->float:        \n",
    "        '''Extracts the nom of Cylinders (mentioned at 3Cylinders)'''\n",
    "        matchCylinders = re.search( r'([.\\d]+)\\s*(?:Cylinder| Cylinder)', x)\n",
    "        if bool(matchCylinders):\n",
    "            cylinders = str(matchCylinders.group(0))\n",
    "            return float(re.findall(r\"[-+]?\\d*\\.*\\d+\", cylinders)[0])\n",
    "        return 0.0\n",
    "\n",
    "    \n",
    "    df['engine_volume'] = df['engine'].apply(extract_capacity)\n",
    "    df['engine_HP'] = df['engine'].apply(extract_horse_power)\n",
    "    df['cylinders'] = df['engine'].apply(extract_cylinders)\n",
    "    return df\n",
    "\n",
    "def detect_starting_number(s: str) -> int:\n",
    "        \"\"\"\n",
    "        Detects if string starts with positive integer and returns value otherwise returns 0\n",
    "        \"\"\"\n",
    "        s = s.lstrip()  \n",
    "        if s and s[0].isdigit():\n",
    "            num = ''\n",
    "            for char in s:\n",
    "                if char.isdigit():\n",
    "                    num += char\n",
    "                else:\n",
    "                    break\n",
    "            return int(num) if int(num) >= 1 else 0\n",
    "        return 0\n",
    "\n",
    "def frequency_encoding(df, columns):\n",
    "        for col in columns:\n",
    "            freq_encoding = df[col].value_counts() / len(df)\n",
    "            name = col + \"_freq\"\n",
    "            df[name] = df[col].map(freq_encoding)\n",
    "        return df\n",
    "\n",
    "def impute_fuel_type(df: pd.DataFrame)->pd.DataFrame:\n",
    "     # Function to extract fuel type from engine column\n",
    "    def extract_fuel_type(engine):\n",
    "        match = re.search(r'(\\w+)\\sFuel', engine)\n",
    "        return match.group(1) if match else None\n",
    "    df['fuel_type'] = df.apply(lambda row: row['fuel_type'] if pd.notna(row['fuel_type']) else extract_fuel_type(row['engine']), axis=1)\n",
    "    return df\n",
    "\n",
    "final_dict = {}\n",
    "def encode_brand(df: pd.DataFrame)->pd.DataFrame:\n",
    "    dick = dict(df.groupby('brand')['price'].mean())\n",
    "    sorted_dict = dict(sorted(dick.items(), key=lambda item: item[1]))\n",
    "    i=1\n",
    "    for x,y in sorted_dict.items():\n",
    "         final_dict[x]=i\n",
    "         i+=1\n",
    "    df['brand_val'] = df['brand'].map(final_dict).fillna(0).astype(int)\n",
    "    return df\n",
    "\n",
    "def fill_missing(df):\n",
    "    df[\"Profession\"] = df[\"Profession\"].fillna(\"Student\")\n",
    "    df[\"Degree\"] = df[\"Degree\"].fillna(\"Unknown\")\n",
    "    df['Pressure'] = df['Work Pressure'].fillna(df['Academic Pressure'])\n",
    "    df['Satisfaction'] = df['Job Satisfaction'].fillna(df['Study Satisfaction'])\n",
    "    df[\"CGPA\"] = df[\"CGPA\"].fillna(0)\n",
    "    return df\n",
    "\n",
    "def encode(df: pd.DataFrame)->pd.DataFrame:\n",
    "    degree = {\n",
    "        \"BCom\": \"B.Com\", \"B.Com\": \"B.Com\", \"B.Comm\": \"B.Com\",\n",
    "        \"B.Tech\": \"B.Tech\", \"BTech\": \"B.Tech\", \"B.T\": \"B.Tech\",\n",
    "        \"BSc\": \"B.Sc\", \"B.Sc\": \"B.Sc\", \"Bachelor of Science\": \"B.Sc\",\n",
    "        \"BArch\": \"B.Arch\", \"B.Arch\": \"B.Arch\",\n",
    "        \"BA\": \"B.A\", \"B.A\": \"B.A\",\n",
    "        \"BBA\": \"BBA\", \"BB\": \"BBA\",\n",
    "        \"BCA\": \"BCA\",\n",
    "        \"BE\": \"BE\",\n",
    "        \"BEd\": \"B.Ed\", \"B.Ed\": \"B.Ed\",\n",
    "        \"BPharm\": \"B.Pharm\", \"B.Pharm\": \"B.Pharm\",\n",
    "        \"BHM\": \"BHM\",\n",
    "        \"LLB\": \"LLB\", \"LL B\": \"LLB\", \"LL BA\": \"LLB\", \"LL.Com\": \"LLB\", \"LLCom\": \"LLB\",\n",
    "        \"MCom\": \"M.Com\", \"M.Com\": \"M.Com\",\n",
    "        \"M.Tech\": \"M.Tech\", \"MTech\": \"M.Tech\", \"M.T\": \"M.Tech\",\n",
    "        \"MSc\": \"M.Sc\", \"M.Sc\": \"M.Sc\", \"Master of Science\": \"M.Sc\",\n",
    "        \"MBA\": \"MBA\",\n",
    "        \"MCA\": \"MCA\",\n",
    "        \"MD\": \"MD\",\n",
    "        \"ME\": \"ME\",\n",
    "        \"MEd\": \"M.Ed\", \"M.Ed\": \"M.Ed\",\n",
    "        \"MArch\": \"M.Arch\", \"M.Arch\": \"M.Arch\",\n",
    "        \"MPharm\": \"M.Pharm\", \"M.Pharm\": \"M.Pharm\",\n",
    "        \"MA\": \"MA\", \"M.A\": \"MA\",\n",
    "        \"MPA\": \"MPA\",\n",
    "        \"LLM\": \"LLM\",\n",
    "        \"PhD\": \"PhD\",\n",
    "        \"MBBS\": \"MBBS\",\n",
    "        \"CA\": \"CA\",\n",
    "        \"Class 12\": \"Class 12\", \"12th\": \"Class 12\",\n",
    "        \"Class 11\": \"Class 11\", \"11th\": \"Class 11\"\n",
    "    }\n",
    "    df['Degree'] = df['Degree'].map(degree)\n",
    "    df['Gender'] = df['Gender'].map({'Male': 1, 'Female':0}).fillna(0).astype(int)\n",
    "    df['Working Professional or Student'] = df['Working Professional or Student'].map({'Working Professional': 1, 'Student':0}).fillna(0).astype(int)\n",
    "    df[\"Suicide\"]=df[\"Have you ever had suicidal thoughts ?\"].map({'Yes': 1, 'No':0}).fillna(0).astype(int)\n",
    "    ordinal_mapping = { 'More Healthy': 1, 'Healthy': 2, 'Less Healthy': 3, 'Moderate': 4, 'Less than Healthy': 5, 'No Healthy': 6, 'Unhealthy': 7}\n",
    "    df['Dietary Habits'] = df['Dietary Habits'].apply(lambda x: ordinal_mapping.get(x, 0))\n",
    "    df['Family History of Mental Illness'] = df['Family History of Mental Illness'].map({'Yes': 1, 'No':0}).fillna(0).astype(int)\n",
    "    sleep_duration_mapping = { '1-2 hours': 1, '2-3 hours': 2, '3-4 hours': 3, '4-5 hours': 4, 'Less than 5 hours': 4, 'than 5 hours': 4, '5-6 hours': 5, '6-7 hours': 6, '7-8 hours': 7, '8-9 hours': 8, '8 hours': 8, '9-11 hours': 9, '9-5 hours': 9, '9-6 hours': 9, '10-11 hours': 10, '10-6 hours': 10, 'More than 8 hours': 11}\n",
    "    df['Sleep Duration'] = df['Sleep Duration'].map(sleep_duration_mapping)\n",
    "    df['Stress'] = df[\"Pressure\"]+df[\"Financial Stress\"]-df[\"Satisfaction\"]\n",
    "    return df\n",
    "\n",
    "def pre_process(df):\n",
    "    df['Age_bin'] = KBinsDiscretizer(n_bins=15, encode='ordinal', strategy='quantile').fit_transform(df[['Age']])\n",
    "    df = fill_missing(df)\n",
    "    df = encode(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = pre_process(df)\n",
    "df = remove_outliers(df, outlier_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "\n",
    "df.to_csv(\"df.csv\", index=False)\n",
    "\n",
    "def gen_eda():\n",
    "    profile = ProfileReport(\n",
    "        pd.concat([df], axis=1),\n",
    "        title=\"Pandas Profiling Report\",\n",
    "        explorative=True,\n",
    "    )\n",
    "    profile.to_file(\"pandas_profiling_report.html\")\n",
    "\n",
    "\n",
    "# gen_eda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126630, 22)\n"
     ]
    }
   ],
   "source": [
    "# Define features and target\n",
    "def get_X_Y(df):\n",
    "    X = df.drop(columns=[\"Name\", \"id\", \"Depression\"])\n",
    "    Y = df[\"Depression\"]\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "X, Y = get_X_Y(df)\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.10, random_state=5\n",
    ")\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of categorical column names\n",
    "numerical_features = X_train.columns\n",
    "categories_order = {\n",
    "    \"Age_bin\": sorted(list(df[\"Age_bin\"].unique())),\n",
    "    \"Gender\": sorted(list(df[\"Gender\"].unique())),\n",
    "    \"Suicide\": sorted(list(df[\"Suicide\"].unique())),\n",
    "    \"Pressure\": sorted(list(df[\"Pressure\"].unique())),\n",
    "    \"Dietary Habits\": sorted(list(df[\"Dietary Habits\"].unique())),\n",
    "    \"Satisfaction\": sorted(list(df[\"Satisfaction\"].unique())),\n",
    "    \"Family History of Mental Illness\": sorted(list(df[\"Family History of Mental Illness\"].unique())),\n",
    "    \"Financial Stress\": sorted(list(df[\"Financial Stress\"].unique())),\n",
    "    \"Sleep Duration\": sorted(list(df[\"Sleep Duration\"].unique())),\n",
    "    \"Work/Study Hours\": sorted(list(df[\"Work/Study Hours\"].unique())),\n",
    "    \"Stress\": sorted(list(df[\"Stress\"].unique())),\n",
    "}\n",
    "categorical_feat_ord = list(categories_order.keys())\n",
    "categorical_feat_nom = [\"City\", \"Degree\"]\n",
    "numerical_features_1 = [\"CGPA\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate transformers for categorical and numerical features\n",
    "\n",
    "# trf = FunctionTransformer(np.log1p, validate=True)\n",
    "# trf = PowerTransformer()\n",
    "# trf = FunctionTransformer(np.sqrt, validate=True)\n",
    "trf = FunctionTransformer(np.sin)\n",
    "# trf = StandardScaler()\n",
    "# trf = MinMaxScaler()\n",
    "\n",
    "numerical_transformer_1 = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"log\", trf),\n",
    "    ]\n",
    ")\n",
    "categorical_transformer_onehot = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "# Create the categorical transformer for ordinal features with an imputer\n",
    "categorical_transformer_ordinal = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  \n",
    "        (\n",
    "            \"ordinal\",\n",
    "            OrdinalEncoder(\n",
    "                categories=[categories_order[col] for col in categorical_feat_ord],\n",
    "                handle_unknown=\"use_encoded_value\",\n",
    "                unknown_value=-1,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best hyperparameters: OrderedDict([('model__colsample_bytree', 0.6332206326326986), ('model__learning_rate', 0.04901812665199213), ('model__max_depth', 10), ('model__min_child_samples', 14), ('model__n_estimators', 299), ('model__num_leaves', 15), ('model__subsample', 0.6687935314288118)])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", categorical_transformer_onehot, categorical_feat_nom),\n",
    "        (\"cat_1\", categorical_transformer_ordinal, categorical_feat_ord),\n",
    "        (\"num\", numerical_transformer_1, numerical_features_1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = LGBMClassifier(verbose=-1)\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([(\"preprocessor\", preprocessor),(\"model\", model)])\n",
    "\n",
    "# pipeline.fit(X_train, Y_train)\n",
    "\n",
    "param_grid = {\n",
    "    \"model__n_estimators\": Integer(50, 300),\n",
    "    \"model__max_depth\": Integer(3, 10),\n",
    "    \"model__learning_rate\": Real(0.005, 0.2, prior='log-uniform'),\n",
    "    \"model__num_leaves\": Integer(15, 125),\n",
    "    \"model__min_child_samples\": Integer(5, 30),\n",
    "    \"model__subsample\": Real(0.6, 1.0, prior='uniform'),\n",
    "    \"model__colsample_bytree\": Real(0.6, 1.0, prior='uniform')\n",
    "}\n",
    "bayes_search = BayesSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    verbose=-1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "# Fit the model with hyperparameter tuning\n",
    "bayes_search.fit(X_train, Y_train)\n",
    "\n",
    "# Get the best parameters and model\n",
    "best_pipeline = bayes_search.best_estimator_\n",
    "pipeline = best_pipeline\n",
    "print(\"Best hyperparameters:\", bayes_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9367448471926084\n",
      "Cross-validation accuracy: 0.934044065387349\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the tuned model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Cross-validation accuracy:\", cross_val_score(pipeline, X_test, Y_test, cv=3, scoring=\"accuracy\").mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved as 'submission.csv'\n"
     ]
    }
   ],
   "source": [
    "# Save the best model\n",
    "with open(\"best_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pipeline, f)\n",
    "\n",
    "def pre_process_test(df):\n",
    "    df = fill_missing(df)\n",
    "    df = encode(df)\n",
    "    age_bin_mapping = dict(zip(X_train['Age'], X_train['Age_bin']))\n",
    "    df['Age_bin'] = df['Age'].map(age_bin_mapping).fillna(-1)\n",
    "    return df\n",
    "\n",
    "# Generate submission as before\n",
    "def generate_submission(test_file):\n",
    "    df = pd.read_csv(test_file)\n",
    "    df.replace(\"\", np.nan, inplace=True)\n",
    "    df = pre_process_test(df)\n",
    "    filtered_df = df[X_train.columns]\n",
    "    predictions = pipeline.predict(filtered_df)\n",
    "    original_df = pd.read_csv(test_file)\n",
    "    original_df[\"Target\"] = predictions\n",
    "    submission_df = original_df[[\"id\", \"Target\"]]\n",
    "    submission_df.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"Submission file saved as 'submission.csv'\")\n",
    "\n",
    "# Run submission generation\n",
    "test_file = \"test.csv\"\n",
    "generate_submission(test_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
